#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Labeled LDA using nltk.corpus.reuters as dataset
# This code is available under the MIT License.
# (c)2013 Nakatani Shuyo / Cybozu Labs Inc.

import sys, string, random, numpy

class LLDA:
    def __init__(self, K, alpha, beta):
        self.alpha = alpha
        self.beta = beta

    def term_to_id(self, term):
        if term not in self.vocas_id:
            voca_id = len(self.vocas)
            self.vocas_id[term] = voca_id
            self.vocas.append(term)
        else:
            voca_id = self.vocas_id[term]
        return voca_id

    def complement_label(self, label):
        if not label: return numpy.ones(len(self.labelmap))
        vec = numpy.zeros(len(self.labelmap))
        for x in label: vec[self.labelmap[x]] = 1.0
        return vec

    def set_corpus(self, labelset, corpus, labels):
        self.labelmap = dict(zip(labelset, range(len(labelset))))
        self.K = len(self.labelmap)

	self.vocas = []
        self.vocas_id = dict()

        self.labels = numpy.array([self.complement_label(label) for label in labels])
        self.docs = [[self.term_to_id(term) for term in doc] for doc in corpus]
	M = len(corpus)
        V = len(self.vocas)

        self.z_m_n = []
        self.n_m_z = numpy.zeros((M, self.K), dtype=int)
        self.n_z_t = numpy.zeros((self.K, V), dtype=int)
        self.n_z = numpy.zeros(self.K, dtype=int)
        for m, doc, label in zip(range(M), self.docs, self.labels):
            N_m = len(doc)
            z_n = [numpy.random.multinomial(1, label / label.sum()).argmax() for x in range(N_m)]
            self.z_m_n.append(z_n)
            for t, z in zip(doc, z_n):
                self.n_m_z[m, z] += 1
                self.n_z_t[z, t] += 1
                self.n_z[z] += 1
        print self.n_z_t.shape
	sys.exit()


    def inference_query(self, testdoc, test_doc, testlabel, test_n_z, test_z_n):
        V = len(self.vocas)
	
	for t, z in zip(testdoc, test_z_n):
	    test_n_z[z] += 1  
        
	for n in range(len(test_doc)):
            t = testdoc[n]
	    if (t< V):
                z = test_z_n[n]
            	    
                denom_a = test_n_z.sum() + self.K * self.alpha
                denom_b = self.n_z_t.sum(axis=1) + V * self.beta
            
		p_z = testlabel * (self.n_z_t[:, t] + self.beta) / denom_b * (test_n_z + self.alpha) / denom_a
                new_z = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()
		newvalue = p_z[new_z]
	    else:
		z = test_z_n[n]
		
            	denom_a = test_n_z.sum() + self.K * self.alpha
            	denom_b = self.n_z_t.sum(axis=1) + V * self.beta

            	p_z = testlabel * (self.beta) / denom_b * (test_n_z + self.alpha) / denom_a
		
            	new_z = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()
		newvalue = p_z[new_z]
	    test_z_n[n] = new_z
	return test_n_z 

    def inference(self):
        V = len(self.vocas)
        for m, doc, label in zip(range(len(self.docs)), self.docs, self.labels):
            for n in range(len(doc)):
                t = doc[n]
		
                z = self.z_m_n[m][n]
		
                self.n_m_z[m, z] -= 1
                self.n_z_t[z, t] -= 1
                self.n_z[z] -= 1

                denom_a = self.n_m_z[m].sum() + self.K * self.alpha
                denom_b = self.n_z_t.sum(axis=1) + V * self.beta
                p_z = label * (self.n_z_t[:, t] + self.beta) / denom_b * (self.n_m_z[m] + self.alpha) / denom_a
                new_z = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()
		
                self.z_m_n[m][n] = new_z
                self.n_m_z[m, new_z] += 1
                self.n_z_t[new_z, t] += 1
                self.n_z[new_z] += 1
	    
    def phi(self):
        V = len(self.vocas)
        return (self.n_z_t + self.beta) / (self.n_z[:, numpy.newaxis] + V * self.beta)

    def theta(self):
        """document-topic distribution"""
        n_alpha = self.n_m_z + self.labels * self.alpha
        return n_alpha / n_alpha.sum(axis=1)[:, numpy.newaxis]

    def perplexity(self, docs=None):
        if docs == None: docs = self.docs
        phi = self.phi()
        thetas = self.theta()
	
        log_per = N = 0
        for doc, theta in zip(docs, thetas):
            for w in doc:
                log_per -= numpy.log(numpy.inner(phi[:,w], theta))
            N += len(doc)
        return numpy.exp(log_per / N)
